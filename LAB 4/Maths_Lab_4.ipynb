{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Maths Lab - 4\n","\n","## Teammates:\n","#### Akshita Potdar - 702768460\n","#### Kalyani Khandait - 702768391\n","#### Logesh Gangadharan - 702723890\n"],"metadata":{"id":"tMS03qkRdQA-"}},{"cell_type":"markdown","source":["##  Problem -1\n","\n","Use the following steps to convince yourself that the theorem holds:\n","1 Generate X, y at random.\n","2 Solve the lasso problem with λ = λmax.\n","3 Compute M = max |β∗\n","i |.\n","Repeat the above experiment N times for a large value of N . If Mi\n","is the value of M obtained during the i-th experiment, compute\n","max\n","i=1,...,N Mi\n","and make sure M = 0 (up to numerical precision)."],"metadata":{"id":"Eu_GUPzEdgwz"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FSyKsSWdO5Q","executionInfo":{"status":"ok","timestamp":1727923526551,"user_tz":240,"elapsed":366,"user":{"displayName":"Akshita Raveendranath Potdar","userId":"12039875111989750810"}},"outputId":"298e21ec-e3f1-4a53-c777-95aabcc943ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Max M across 100 experiments: 0.0000000000\n"]}],"source":["import numpy as np\n","from sklearn.linear_model import Lasso\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","# Generating random data for linear regression\n","np.random.seed(42)\n","x = 2 * np.random.rand(100, 1)\n","y = 4 + 3 * x + np.random.randn(100, 1)\n","\n","n=40\n","p=10\n","N=100\n","\n","coef=[]\n","\n","for _ in range(N):\n","  lambda_max = (1/n) * np.max(np.abs(x.T @y))\n","  lasso = Lasso(alpha = lambda_max, fit_intercept= False)\n","  lasso.fit(x,y)\n","\n","  # for the formula M= max of Beta star\n","  beta_star = lasso.coef_\n","\n","  M = np.max(np.abs(beta_star))\n","  coef.append(M)\n","\n","\n","M_max = np.max(coef)\n","print(f'Max M across {N} experiments: {M_max:.10f}')\n"]},{"cell_type":"markdown","source":["## Problem - 2"],"metadata":{"id":"EAEiZIgyTHLw"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.linear_model import LinearRegression\n","\n","def coordinate_descent(A, y, tol=1e-6, positive=False, max_iter=1000):\n","    m, n = A.shape\n","    x = np.zeros(n)\n","    residuals = y - A.dot(x)\n","    diff = np.inf\n","    iter_count = 0\n","\n","    while diff > tol and iter_count < max_iter:\n","        x_old = x.copy()\n","        for j in range(n):\n","            # Calculate the partial residual without the contribution of x_j\n","            residual_no_j = residuals + A[:, j] * x[j]\n","            # Update x_j based on the partial residual and constraint\n","            rho = np.dot(A[:, j], residual_no_j)\n","            if positive:\n","                x[j] = max(0, rho / np.dot(A[:, j], A[:, j]))\n","            else:\n","                x[j] = rho / np.dot(A[:, j], A[:, j])\n","            # Update residuals\n","            residuals = residual_no_j - A[:, j] * x[j]\n","\n","        diff = np.linalg.norm(x - x_old)\n","        iter_count += 1\n","\n","    return x\n","\n","\n","np.random.seed(0)\n","A = np.random.randn(100, 10)\n","y = A.dot(np.random.randn(10)) + np.random.randn(100)\n","\n","# Coordinate descent solution with positive coefficients\n","x_cd_positive = coordinate_descent(A, y, positive=True)\n","\n","# Linear Regression for comparison\n","lr = LinearRegression(fit_intercept=False)\n","lr.fit(A, y)\n","x_lr = lr.coef_\n","x_lr_positive = np.maximum(x_lr, 0)\n","\n","# Print results\n","print(\"Coordinate Descent Positive Coefficients:\", x_cd_positive)\n","print(\"Linear Regression Coefficients\", x_lr_positive)\n","print(\"Difference between Coordinate Descent Positive and Adjusted Linear Regression:\", np.linalg.norm(x_cd_positive - x_lr_positive))\n"],"metadata":{"id":"gHj8CffPfDLY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728835529393,"user_tz":240,"elapsed":193,"user":{"displayName":"Akshita Raveendranath Potdar","userId":"12039875111989750810"}},"outputId":"23bd79fc-9c37-49dc-c08d-079fe375c2d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Coordinate Descent Positive Coefficients: [0.44908691 0.81784225 0.         0.25489193 0.4126498  0.31741838\n"," 0.62476308 0.         0.         0.21062992]\n","Linear Regression Coefficients [0.60384959 0.83015874 0.         0.12480557 0.28558375 0.26861244\n"," 0.53156047 0.         0.         0.28404853]\n","Difference between Coordinate Descent Positive and Adjusted Linear Regression: 0.2713493376607383\n"]}]},{"cell_type":"markdown","source":["## Problem - 3"],"metadata":{"id":"U1wGQacVRN1U"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.linear_model import Lasso\n","\n","#function for soft thresholding\n","def soft_thresholding(rho, lambda_):\n","    if rho < -lambda_:\n","        return (rho + lambda_)\n","    elif rho > lambda_:\n","        return (rho - lambda_)\n","    else:\n","        return 0\n","\n","\n","#fucntion for coordinate-descent algorithm for lasso regression\n","def lasso_coordinate_descent(X, y, lambda_, max_iter=1000, tol=1e-6):\n","    n, p = X.shape\n","    beta = np.zeros(p)\n","\n","    for iteration in range(max_iter):\n","        beta_old = np.copy(beta)\n","\n","        for j in range(p):\n","\n","            residual = y - (X @ beta) + X[:, j] * beta[j]\n","\n","            # Calculating rho\n","            rho = np.dot(X[:, j], residual)\n","\n","\n","            beta[j] = soft_thresholding(rho, lambda_) / np.sum(X[:, j]**2)\n","\n","\n","        if np.sum(np.abs(beta - beta_old)) < tol:\n","            break\n","\n","    return beta\n","\n","# Generating random data\n","np.random.seed(0)\n","n, p = 50, 10  # 50 samples and 10 features\n","X = np.random.randn(n, p)  # Random feature matrix\n","y = np.random.randn(n)     # Random response vector\n","\n","\n","alpha = 0.1  # Regularization parameter\n","lambda_ = alpha * n\n","beta_custom = lasso_coordinate_descent(X, y, lambda_)\n","\n","# Sklearn Lasso implementation\n","lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000, tol=1e-6)\n","lasso.fit(X, y)\n","beta_sklearn = lasso.coef_\n","\n","#putput the coefficients\n","print(\"Custom implementation coefficients (larger dataset):\\n\", beta_custom)\n","print(\"Scikit-learn Lasso coefficients (larger dataset):\\n\", beta_sklearn)\n","\n","#comparing the difference between the coefficients\n","difference = np.linalg.norm(beta_custom - beta_sklearn)\n","print(f\"Difference between custom and sklearn coefficients: {difference:.10f}\")\n","\n"],"metadata":{"id":"b_b8UmPOq1Jc","executionInfo":{"status":"ok","timestamp":1727924137758,"user_tz":240,"elapsed":304,"user":{"displayName":"Akshita Raveendranath Potdar","userId":"12039875111989750810"}},"outputId":"548e0aff-2dfc-49f2-c70f-71b11cf3ae08","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Custom implementation coefficients (larger dataset):\n"," [ 0.1208606   0.          0.0763886   0.         -0.10401496 -0.00296079\n","  0.1302818   0.05173474  0.          0.04710325]\n","Scikit-learn Lasso coefficients (larger dataset):\n"," [ 0.12086055 -0.          0.07638862  0.         -0.10401498 -0.00296079\n","  0.13028181  0.05173474 -0.          0.04710325]\n","Difference between custom and sklearn coefficients: 0.0000000580\n"]}]}]}